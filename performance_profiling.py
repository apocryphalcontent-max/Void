"""
Performance Profiling System for Void-State Tools

Advanced performance analysis beyond basic benchmarking:
- Flamegraph generation
- Hotspot detection
- Memory profiling
- Cache analysis
- Lock contention analysis
- Performance regression detection

References:
- "Systems Performance" (Gregg, 2020)
- "The Art of Computer Systems Performance Analysis" (Jain, 1991)
"""

from typing import Dict, List, Tuple, Optional, Any, Callable, Set
from dataclasses import dataclass, field
from collections import defaultdict, deque
import time
import sys
import gc
import threading
from contextlib import contextmanager
import traceback


# ============================================================================
# CALL STACK PROFILER
# ============================================================================

@dataclass
class ProfileFrame:
    """Single frame in profiling stack"""
    function_name: str
    filename: str
    line_number: int
    start_time: float
    end_time: Optional[float] = None
    cumulative_time: float = 0.0
    self_time: float = 0.0
    call_count: int = 0
    children: Dict[str, 'ProfileFrame'] = field(default_factory=dict)


class StackProfiler:
    """
    Statistical profiling via stack sampling.
    
    Periodically samples call stack to build performance profile.
    Low overhead (~1-5% typically).
    """
    
    def __init__(self, sample_interval: float = 0.001):
        """
        Args:
            sample_interval: Time between samples in seconds
        """
        self.sample_interval = sample_interval
        self.samples: List[List[Tuple[str, str, int]]] = []
        self.running = False
        self.thread: Optional[threading.Thread] = None
    
    def start(self) -> None:
        """Start profiling"""
        if self.running:
            return
        
        self.running = True
        self.samples = []
        self.thread = threading.Thread(target=self._sample_loop, daemon=True)
        self.thread.start()
    
    def stop(self) -> None:
        """Stop profiling"""
        self.running = False
        if self.thread:
            self.thread.join(timeout=1.0)
    
    def _sample_loop(self) -> None:
        """Sample loop running in separate thread"""
        while self.running:
            # Get current stack
            stack = []
            frame = sys._getframe()
            
            while frame:
                code = frame.f_code
                stack.append((
                    code.co_name,
                    code.co_filename,
                    frame.f_lineno
                ))
                frame = frame.f_back
            
            self.samples.append(stack)
            time.sleep(self.sample_interval)
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Get profiling statistics.
        
        Returns dict with:
        - total_samples: Total number of samples
        - functions: Dict mapping function -> sample count
        - call_trees: Reconstructed call trees
        """
        if not self.samples:
            return {
                "total_samples": 0,
                "functions": {},
                "call_trees": []
            }
        
        # Count function occurrences
        function_counts: Dict[str, int] = defaultdict(int)
        for stack in self.samples:
            for func_name, _, _ in stack:
                function_counts[func_name] += 1
        
        # Build call tree
        root = self._build_call_tree()
        
        return {
            "total_samples": len(self.samples),
            "functions": dict(function_counts),
            "call_tree": root
        }
    
    def _build_call_tree(self) -> ProfileFrame:
        """Build call tree from samples"""
        root = ProfileFrame("root", "", 0, 0.0)
        
        for stack in self.samples:
            current = root
            for func_name, filename, line_no in reversed(stack):
                key = f"{func_name}@{filename}:{line_no}"
                if key not in current.children:
                    current.children[key] = ProfileFrame(
                        func_name, filename, line_no, 0.0
                    )
                current = current.children[key]
                current.call_count += 1
        
        return root
    
    def generate_flamegraph(self) -> str:
        """
        Generate flamegraph data.
        
        Returns folded stack traces suitable for flamegraph.pl
        """
        lines = []
        for stack in self.samples:
            # Reverse stack (root at bottom)
            stack_str = ";".join(
                f"{func}@{filename.split('/')[-1]}"
                for func, filename, _ in reversed(stack)
            )
            lines.append(f"{stack_str} 1")
        
        return "\n".join(lines)


# ============================================================================
# MEMORY PROFILER
# ============================================================================

@dataclass
class MemorySnapshot:
    """Snapshot of memory usage"""
    timestamp: float
    total_bytes: int
    objects_by_type: Dict[str, int]
    largest_objects: List[Tuple[str, int]]  # (type, size) pairs


class MemoryProfiler:
    """
    Memory profiling and leak detection.
    
    Tracks memory allocations and identifies growth patterns.
    """
    
    def __init__(self):
        self.snapshots: List[MemorySnapshot] = []
        self.baseline: Optional[MemorySnapshot] = None
    
    def take_snapshot(self) -> MemorySnapshot:
        """Take a memory snapshot"""
        gc.collect()  # Force collection
        
        # Count objects by type
        objects_by_type: Dict[str, int] = defaultdict(int)
        largest_objects: List[Tuple[str, int]] = []
        
        for obj in gc.get_objects():
            obj_type = type(obj).__name__
            objects_by_type[obj_type] += 1
            
            # Estimate size
            try:
                size = sys.getsizeof(obj)
                if size > 1024:  # > 1KB
                    largest_objects.append((obj_type, size))
            except:
                pass
        
        # Sort largest objects
        largest_objects.sort(key=lambda x: x[1], reverse=True)
        largest_objects = largest_objects[:20]
        
        snapshot = MemorySnapshot(
            timestamp=time.time(),
            total_bytes=self._get_total_memory(),
            objects_by_type=dict(objects_by_type),
            largest_objects=largest_objects
        )
        
        self.snapshots.append(snapshot)
        return snapshot
    
    def set_baseline(self) -> None:
        """Set current memory state as baseline"""
        self.baseline = self.take_snapshot()
    
    def get_growth(self) -> Dict[str, int]:
        """
        Get memory growth since baseline.
        
        Returns dict mapping type -> count difference
        """
        if not self.baseline or not self.snapshots:
            return {}
        
        current = self.snapshots[-1]
        growth = {}
        
        all_types = set(self.baseline.objects_by_type.keys()) | set(current.objects_by_type.keys())
        
        for obj_type in all_types:
            baseline_count = self.baseline.objects_by_type.get(obj_type, 0)
            current_count = current.objects_by_type.get(obj_type, 0)
            diff = current_count - baseline_count
            if diff != 0:
                growth[obj_type] = diff
        
        return growth
    
    def detect_leaks(self, threshold: int = 100) -> List[str]:
        """
        Detect potential memory leaks.
        
        Returns list of types that have grown significantly.
        """
        if len(self.snapshots) < 2:
            return []
        
        # Compare recent snapshots
        recent = self.snapshots[-5:] if len(self.snapshots) >= 5 else self.snapshots
        
        if len(recent) < 2:
            return []
        
        first = recent[0]
        last = recent[-1]
        
        leaks = []
        
        for obj_type in last.objects_by_type:
            first_count = first.objects_by_type.get(obj_type, 0)
            last_count = last.objects_by_type[obj_type]
            
            if last_count - first_count > threshold:
                leaks.append(obj_type)
        
        return leaks
    
    def _get_total_memory(self) -> int:
        """Get total memory usage in bytes"""
        try:
            import resource
            return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss * 1024
        except:
            return 0


# ============================================================================
# PERFORMANCE REGRESSION DETECTION
# ============================================================================

@dataclass
class PerformanceMetric:
    """Single performance metric measurement"""
    name: str
    value: float
    timestamp: float
    metadata: Dict[str, Any] = field(default_factory=dict)


class RegressionDetector:
    """
    Detect performance regressions using statistical methods.
    
    Uses changepoint detection and hypothesis testing.
    """
    
    def __init__(self, window_size: int = 50, sensitivity: float = 2.0):
        """
        Args:
            window_size: Number of recent samples to consider
            sensitivity: Number of standard deviations for threshold
        """
        self.window_size = window_size
        self.sensitivity = sensitivity
        self.metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=window_size))
    
    def add_measurement(self, name: str, value: float, **metadata) -> None:
        """Add a performance measurement"""
        metric = PerformanceMetric(
            name=name,
            value=value,
            timestamp=time.time(),
            metadata=metadata
        )
        self.metrics[name].append(metric)
    
    def check_regression(self, name: str) -> Optional[Dict[str, Any]]:
        """
        Check if metric has regressed.
        
        Returns regression info if detected, None otherwise.
        """
        if name not in self.metrics:
            return None
        
        measurements = self.metrics[name]
        if len(measurements) < 10:
            return None
        
        values = [m.value for m in measurements]
        
        # Split into baseline and current
        split = len(values) // 2
        baseline = values[:split]
        current = values[split:]
        
        # Compute statistics
        import statistics
        
        baseline_mean = statistics.mean(baseline)
        baseline_std = statistics.stdev(baseline) if len(baseline) > 1 else 0
        current_mean = statistics.mean(current)
        
        # Check for regression
        threshold = baseline_mean + self.sensitivity * baseline_std
        
        if current_mean > threshold:
            return {
                "metric": name,
                "baseline_mean": baseline_mean,
                "current_mean": current_mean,
                "threshold": threshold,
                "regression_percent": ((current_mean - baseline_mean) / baseline_mean) * 100,
                "detected_at": time.time()
            }
        
        return None
    
    def get_summary(self) -> Dict[str, Dict[str, float]]:
        """Get summary statistics for all metrics"""
        import statistics
        
        summary = {}
        
        for name, measurements in self.metrics.items():
            if len(measurements) < 2:
                continue
            
            values = [m.value for m in measurements]
            
            summary[name] = {
                "mean": statistics.mean(values),
                "median": statistics.median(values),
                "stdev": statistics.stdev(values) if len(values) > 1 else 0,
                "min": min(values),
                "max": max(values),
                "count": len(values)
            }
        
        return summary


# ============================================================================
# LOCK CONTENTION ANALYZER
# ============================================================================

@dataclass
class LockEvent:
    """Lock acquisition/release event"""
    lock_id: str
    thread_id: int
    event_type: str  # "acquire" or "release"
    timestamp: float
    wait_time: Optional[float] = None


class LockContentionAnalyzer:
    """
    Analyze lock contention patterns.
    
    Identifies heavily contended locks and blocking relationships.
    """
    
    def __init__(self):
        self.events: List[LockEvent] = []
        self.active_locks: Dict[str, Tuple[int, float]] = {}  # lock_id -> (thread_id, acquire_time)
        self.lock = threading.Lock()
    
    def record_acquire(self, lock_id: str, wait_time: float = 0.0) -> None:
        """Record lock acquisition"""
        with self.lock:
            thread_id = threading.get_ident()
            event = LockEvent(
                lock_id=lock_id,
                thread_id=thread_id,
                event_type="acquire",
                timestamp=time.time(),
                wait_time=wait_time
            )
            self.events.append(event)
            self.active_locks[lock_id] = (thread_id, event.timestamp)
    
    def record_release(self, lock_id: str) -> None:
        """Record lock release"""
        with self.lock:
            thread_id = threading.get_ident()
            event = LockEvent(
                lock_id=lock_id,
                thread_id=thread_id,
                event_type="release",
                timestamp=time.time()
            )
            self.events.append(event)
            if lock_id in self.active_locks:
                del self.active_locks[lock_id]
    
    def get_contention_stats(self) -> Dict[str, Any]:
        """Get lock contention statistics"""
        if not self.events:
            return {}
        
        # Group by lock
        by_lock: Dict[str, List[LockEvent]] = defaultdict(list)
        for event in self.events:
            by_lock[event.lock_id].append(event)
        
        stats = {}
        
        for lock_id, events in by_lock.items():
            acquires = [e for e in events if e.event_type == "acquire"]
            
            if not acquires:
                continue
            
            wait_times = [e.wait_time for e in acquires if e.wait_time is not None]
            
            import statistics
            
            stats[lock_id] = {
                "acquire_count": len(acquires),
                "avg_wait_time": statistics.mean(wait_times) if wait_times else 0,
                "max_wait_time": max(wait_times) if wait_times else 0,
                "contention_score": sum(wait_times) if wait_times else 0
            }
        
        return stats
    
    def get_most_contended(self, top_n: int = 5) -> List[Tuple[str, float]]:
        """Get most contended locks"""
        stats = self.get_contention_stats()
        
        # Sort by contention score
        sorted_locks = sorted(
            stats.items(),
            key=lambda x: x[1]["contention_score"],
            reverse=True
        )
        
        return [(lock_id, data["contention_score"]) 
                for lock_id, data in sorted_locks[:top_n]]


# ============================================================================
# CACHE PERFORMANCE ANALYZER
# ============================================================================

@dataclass
class CacheStats:
    """Cache performance statistics"""
    hits: int = 0
    misses: int = 0
    evictions: int = 0
    avg_lookup_time: float = 0.0
    hit_rate: float = 0.0


class CacheAnalyzer:
    """
    Analyze cache performance.
    
    Tracks hit rates, miss patterns, and optimal cache sizes.
    """
    
    def __init__(self):
        self.stats_by_cache: Dict[str, CacheStats] = {}
        self.access_history: Dict[str, List[Tuple[str, bool, float]]] = defaultdict(list)
    
    def record_access(self, cache_id: str, key: str, hit: bool, lookup_time: float) -> None:
        """Record cache access"""
        if cache_id not in self.stats_by_cache:
            self.stats_by_cache[cache_id] = CacheStats()
        
        stats = self.stats_by_cache[cache_id]
        
        if hit:
            stats.hits += 1
        else:
            stats.misses += 1
        
        # Update hit rate
        total = stats.hits + stats.misses
        stats.hit_rate = stats.hits / total if total > 0 else 0
        
        # Update avg lookup time (running average)
        n = total
        stats.avg_lookup_time = ((n - 1) * stats.avg_lookup_time + lookup_time) / n
        
        # Record in history
        self.access_history[cache_id].append((key, hit, lookup_time))
    
    def record_eviction(self, cache_id: str) -> None:
        """Record cache eviction"""
        if cache_id in self.stats_by_cache:
            self.stats_by_cache[cache_id].evictions += 1
    
    def get_stats(self, cache_id: str) -> Optional[CacheStats]:
        """Get statistics for a cache"""
        return self.stats_by_cache.get(cache_id)
    
    def analyze_working_set(self, cache_id: str, window_size: int = 100) -> Set[str]:
        """
        Analyze working set (frequently accessed keys).
        
        Returns set of keys in working set.
        """
        if cache_id not in self.access_history:
            return set()
        
        history = self.access_history[cache_id]
        recent = history[-window_size:] if len(history) >= window_size else history
        
        # Count key frequencies
        key_counts: Dict[str, int] = defaultdict(int)
        for key, _, _ in recent:
            key_counts[key] += 1
        
        # Return keys accessed more than once
        return {key for key, count in key_counts.items() if count > 1}


# ============================================================================
# INTEGRATED PERFORMANCE MONITOR
# ============================================================================

class PerformanceMonitor:
    """
    Integrated performance monitoring system.
    
    Combines all profiling subsystems.
    """
    
    def __init__(self):
        self.stack_profiler = StackProfiler()
        self.memory_profiler = MemoryProfiler()
        self.regression_detector = RegressionDetector()
        self.lock_analyzer = LockContentionAnalyzer()
        self.cache_analyzer = CacheAnalyzer()
    
    @contextmanager
    def profile_section(self, name: str):
        """Context manager for profiling a code section"""
        start_time = time.time()
        
        try:
            yield
        finally:
            duration = time.time() - start_time
            self.regression_detector.add_measurement(name, duration)
    
    def start_profiling(self) -> None:
        """Start all profilers"""
        self.stack_profiler.start()
        self.memory_profiler.set_baseline()
    
    def stop_profiling(self) -> None:
        """Stop all profilers"""
        self.stack_profiler.stop()
    
    def generate_report(self) -> str:
        """Generate comprehensive performance report"""
        lines = [
            "=" * 70,
            "PERFORMANCE PROFILING REPORT",
            "=" * 70,
            ""
        ]
        
        # Stack profiling
        lines.append("## CALL STACK PROFILE")
        stack_stats = self.stack_profiler.get_stats()
        lines.append(f"Total samples: {stack_stats['total_samples']}")
        lines.append("\nTop functions by sample count:")
        
        top_functions = sorted(
            stack_stats['functions'].items(),
            key=lambda x: x[1],
            reverse=True
        )[:10]
        
        for func, count in top_functions:
            percent = (count / stack_stats['total_samples']) * 100
            lines.append(f"  {func}: {count} ({percent:.1f}%)")
        
        lines.append("")
        
        # Memory profiling
        lines.append("## MEMORY PROFILE")
        if self.memory_profiler.snapshots:
            latest = self.memory_profiler.snapshots[-1]
            lines.append(f"Total memory: {latest.total_bytes / (1024**2):.1f} MB")
            lines.append(f"Object types: {len(latest.objects_by_type)}")
            
            growth = self.memory_profiler.get_growth()
            if growth:
                lines.append("\nMemory growth since baseline:")
                for obj_type, diff in sorted(growth.items(), key=lambda x: abs(x[1]), reverse=True)[:10]:
                    lines.append(f"  {obj_type}: {diff:+d}")
            
            leaks = self.memory_profiler.detect_leaks()
            if leaks:
                lines.append("\nPotential memory leaks:")
                for leak in leaks:
                    lines.append(f"  - {leak}")
        
        lines.append("")
        
        # Performance regressions
        lines.append("## PERFORMANCE REGRESSIONS")
        summary = self.regression_detector.get_summary()
        for name, stats in summary.items():
            regression = self.regression_detector.check_regression(name)
            if regression:
                lines.append(f"\nâš  REGRESSION DETECTED: {name}")
                lines.append(f"  Baseline: {regression['baseline_mean']:.3f}ms")
                lines.append(f"  Current: {regression['current_mean']:.3f}ms")
                lines.append(f"  Change: {regression['regression_percent']:+.1f}%")
        
        lines.append("")
        
        # Lock contention
        lines.append("## LOCK CONTENTION")
        contention_stats = self.lock_analyzer.get_contention_stats()
        if contention_stats:
            most_contended = self.lock_analyzer.get_most_contended()
            lines.append("Most contended locks:")
            for lock_id, score in most_contended:
                lines.append(f"  {lock_id}: {score:.3f}ms total wait time")
        else:
            lines.append("No lock contention data")
        
        lines.append("")
        lines.append("=" * 70)
        
        return "\n".join(lines)
